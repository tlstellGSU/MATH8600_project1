{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "IOError: open(\"/Users/tommystell/.julia/logs/manifest_usage.toml.pid\", 2562, 292): no space left on device (ENOSPC)",
     "output_type": "error",
     "traceback": [
      "IOError: open(\"/Users/tommystell/.julia/logs/manifest_usage.toml.pid\", 2562, 292): no space left on device (ENOSPC)",
      "",
      "Stacktrace:",
      "  [1] uv_error",
      "    @ ./libuv.jl:106 [inlined]",
      "  [2] open(path::String, flags::UInt32, mode::UInt16)",
      "    @ Base.Filesystem ./filesystem.jl:190",
      "  [3] tryopen_exclusive(path::String, mode::UInt16)",
      "    @ FileWatching.Pidfile /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/FileWatching/src/pidfile.jl:217",
      "  [4] open_exclusive(path::String; mode::UInt16, poll_interval::Int64, wait::Bool, stale_age::Int64, refresh::Float64)",
      "    @ FileWatching.Pidfile /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/FileWatching/src/pidfile.jl:244",
      "  [5] open_exclusive",
      "    @ /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/FileWatching/src/pidfile.jl:237 [inlined]",
      "  [6] mkpidlock(at::String, pid::Int32; stale_age::Int64, refresh::Float64, kwopts::@Kwargs{})",
      "    @ FileWatching.Pidfile /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/FileWatching/src/pidfile.jl:68",
      "  [7] mkpidlock",
      "    @ /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/FileWatching/src/pidfile.jl:63 [inlined]",
      "  [8] mkpidlock(f::Pkg.Types.var\"#52#55\"{String, String, Dates.DateTime, String}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64})",
      "    @ FileWatching.Pidfile /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/FileWatching/src/pidfile.jl:93",
      "  [9] mkpidlock",
      "    @ /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/FileWatching/src/pidfile.jl:92 [inlined]",
      " [10] mkpidlock",
      "    @ /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/FileWatching/src/pidfile.jl:90 [inlined]",
      " [11] write_env_usage(source_file::String, usage_filepath::String)",
      "    @ Pkg.Types /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/Pkg/src/Types.jl:534",
      " [12] Pkg.Types.EnvCache(env::Nothing)",
      "    @ Pkg.Types /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/Pkg/src/Types.jl:372",
      " [13] EnvCache",
      "    @ /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/Pkg/src/Types.jl:351 [inlined]",
      " [14] Context",
      "    @ /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/Pkg/src/Types.jl:401 [inlined]",
      " [15] up(pkgs::Vector{Pkg.Types.PackageSpec}; io::IOContext{IO}, kwargs::@Kwargs{})",
      "    @ Pkg.API /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/Pkg/src/API.jl:150",
      " [16] up(pkgs::Vector{Pkg.Types.PackageSpec})",
      "    @ Pkg.API /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/Pkg/src/API.jl:148",
      " [17] up",
      "    @ /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/Pkg/src/API.jl:147 [inlined]",
      " [18] up(pkg::String)",
      "    @ Pkg.API /Applications/Julia-1.11.app/Contents/Resources/julia/share/julia/stdlib/v1.11/Pkg/src/API.jl:146",
      " [19] top-level scope",
      "    @ In[86]:5"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "\n",
    "#Pkg.add(\"Zygote\")\n",
    "\n",
    "using Zygote\n",
    "using Flux\n",
    "using Flux: onehotbatch, onecold, crossentropy\n",
    "using MLDatasets\n",
    "#using CUDA\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = MNIST(split=:train)[:]\n",
    "test_x, test_y = MNIST(split=:test)[:]\n",
    "\n",
    "train_x = float.(train_x) ./ 255.0\n",
    "test_x = float.(test_x) ./ 255.0\n",
    "\n",
    "train_x = reshape(train_x, 28, 28, 1, :);\n",
    "test_x = reshape(test_x, 28, 28, 1, :);\n",
    "\n",
    "train_y = onehotbatch(train_y, 0:9);\n",
    "test_y = onehotbatch(test_y, 0:9);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = Conv((3,3), 1=>16, relu)\n",
    "pool1 = MaxPool((2,2))\n",
    "conv2 = Conv((3,3), 16=>32, relu)\n",
    "pool2 = MaxPool((2,2))\n",
    "conv3 = Conv((3,3), 32=>64, relu)\n",
    "pool3 = MaxPool((2,2))\n",
    "flatten = Flux.flatten\n",
    "dense1 = Dense(64, 128, relu)\n",
    "dense2 = Dense(128, 10)\n",
    "softmax_layer = softmax\n",
    "\n",
    "model = Chain(conv1, pool1, conv2, pool2, conv3, pool3, flatten, dense1, dense2, softmax_layer)\n",
    "\n",
    "loss(x,y) = crossentropy(model(x), y)\n",
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "visualize_layers (generic function with 1 method)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function visualize_layers(model, x)\n",
    "    println(\"Input shape: \", size(x))\n",
    "    x = conv1(x); println(\"After Conv1: \", size(x))\n",
    "    x = pool1(x); println(\"After Pool1: \", size(x))\n",
    "    x = conv2(x); println(\"After Conv2: \", size(x))\n",
    "    x = pool2(x); println(\"After Pool2: \", size(x))\n",
    "    x = flatten(x); println(\"After Flatten: \", size(x))\n",
    "    x = dense1(x); println(\"After Dense1: \", size(x))\n",
    "    x = dense2(x); println(\"After Dense2: \", size(x))\n",
    "    x = softmax_layer(x); println(\"After Softmax: \", size(x))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_model! (generic function with 5 methods)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_model!(model, train_X, train_Y, opt, epochs, batch_size)\n",
    "    data_loader = Flux.DataLoader((train_X, train_Y), batchsize=batch_size, shuffle=true)\n",
    "    \n",
    "    opt_state = Flux.setup(opt, model)  \n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        for (x, y) in data_loader\n",
    "            gs = gradient(() -> Flux.Losses.mse(model(x), y), Flux.params(model))\n",
    "            Flux.update!(opt_state, model, gs)  \n",
    "        end\n",
    "        println(\"Epoch $epoch complete\")\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mImplicit gradients such as `gradient(f, ::Params)` are deprecated in Flux!\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mPlease see the docs for new explicit form.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  caller = train_model!(model::Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, typeof(Flux.flatten), Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}, train_X::Array{Float64, 4}, train_Y::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}, opt::Adam{Float64, Tuple{Float64, Float64}, Float64}, epochs::Int64, batch_size::Int64) at In[84]:8\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main ./In[84]:8\u001b[39m\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "type Grads has no field layers",
     "output_type": "error",
     "traceback": [
      "type Grads has no field layers",
      "",
      "Stacktrace:",
      " [1] #2",
      "   @ ~/.julia/packages/Functors/LbNAu/src/functor.jl:21 [inlined]",
      " [2] ntuple(f::Functors.var\"#2#5\"{Zygote.Grads, Tuple{Symbol}}, n::Int64)",
      "   @ Base ./ntuple.jl:19",
      " [3] functor(T::Type, x::Zygote.Grads)",
      "   @ Functors ~/.julia/packages/Functors/LbNAu/src/functor.jl:21",
      " [4] (::Optimisers.var\"#13#15\"{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, typeof(Flux.flatten), Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}})(x̄::Zygote.Grads)",
      "   @ Optimisers ~/.julia/packages/Optimisers/pOtCV/src/interface.jl:124",
      " [5] map",
      "   @ ./tuple.jl:355 [inlined]",
      " [6] _grads!(dict::IdDict{Optimisers.Leaf, Any}, tree::@NamedTuple{layers::Tuple{@NamedTuple{σ::Tuple{}, weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Array{Float32, 4}, Array{Float32, 4}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, stride::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, dilation::Tuple{Tuple{}, Tuple{}}, groups::Tuple{}}, @NamedTuple{k::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, stride::Tuple{Tuple{}, Tuple{}}}, @NamedTuple{σ::Tuple{}, weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Array{Float32, 4}, Array{Float32, 4}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, stride::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, dilation::Tuple{Tuple{}, Tuple{}}, groups::Tuple{}}, @NamedTuple{k::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, stride::Tuple{Tuple{}, Tuple{}}}, @NamedTuple{σ::Tuple{}, weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Array{Float32, 4}, Array{Float32, 4}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, stride::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, dilation::Tuple{Tuple{}, Tuple{}}, groups::Tuple{}}, @NamedTuple{k::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, stride::Tuple{Tuple{}, Tuple{}}}, Tuple{}, @NamedTuple{weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}, @NamedTuple{weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}, Tuple{}}}, x::Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, typeof(Flux.flatten), Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}, x̄s::Zygote.Grads)",
      "   @ Optimisers ~/.julia/packages/Optimisers/pOtCV/src/interface.jl:124",
      " [7] update!(::@NamedTuple{layers::Tuple{@NamedTuple{σ::Tuple{}, weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Array{Float32, 4}, Array{Float32, 4}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, stride::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, dilation::Tuple{Tuple{}, Tuple{}}, groups::Tuple{}}, @NamedTuple{k::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, stride::Tuple{Tuple{}, Tuple{}}}, @NamedTuple{σ::Tuple{}, weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Array{Float32, 4}, Array{Float32, 4}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, stride::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, dilation::Tuple{Tuple{}, Tuple{}}, groups::Tuple{}}, @NamedTuple{k::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, stride::Tuple{Tuple{}, Tuple{}}}, @NamedTuple{σ::Tuple{}, weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Array{Float32, 4}, Array{Float32, 4}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, stride::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, dilation::Tuple{Tuple{}, Tuple{}}, groups::Tuple{}}, @NamedTuple{k::Tuple{Tuple{}, Tuple{}}, pad::NTuple{4, Tuple{}}, stride::Tuple{Tuple{}, Tuple{}}}, Tuple{}, @NamedTuple{weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}, @NamedTuple{weight::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}, Tuple{}}}, ::Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, typeof(Flux.flatten), Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}, ::Zygote.Grads)",
      "   @ Optimisers ~/.julia/packages/Optimisers/pOtCV/src/interface.jl:74",
      " [8] train_model!(model::Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, MaxPool{2, 4}, typeof(Flux.flatten), Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}, train_X::Array{Float64, 4}, train_Y::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}, opt::Adam{Float64, Tuple{Float64, Float64}, Float64}, epochs::Int64, batch_size::Int64)",
      "   @ Main ./In[84]:9",
      " [9] top-level scope",
      "   @ In[85]:1"
     ]
    }
   ],
   "source": [
    "train_model!(model, train_x, train_y, opt, 5, 64)\n",
    "\n",
    "visualize_layers(model, train_x[:, :, :, 1:1])\n",
    "\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "println(\"Test Accuracy: \", accuracy(test_x, test_y))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
