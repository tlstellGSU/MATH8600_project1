{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m WAV ─ v1.2.0\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.11/Project.toml`\n",
      "  \u001b[90m[8149f6b0] \u001b[39m\u001b[92m+ WAV v1.2.0\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.11/Manifest.toml`\n",
      "  \u001b[90m[8149f6b0] \u001b[39m\u001b[92m+ WAV v1.2.0\u001b[39m\n",
      "\u001b[92m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "   6077.1 ms\u001b[32m  ✓ \u001b[39mWAV\n",
      "   8249.8 ms\u001b[32m  ✓ \u001b[39mPlots → UnitfulExt\n",
      "         \u001b[91m  ✗ \u001b[39mCUDA\n",
      "         \u001b[91m  ✗ \u001b[39mCUDA → EnzymeCoreExt\n",
      "         \u001b[91m  ✗ \u001b[39m\u001b[90mAtomix → AtomixCUDAExt\u001b[39m\n",
      "         \u001b[91m  ✗ \u001b[39m\u001b[90mArrayInterface → ArrayInterfaceCUDAExt\u001b[39m\n",
      "         \u001b[91m  ✗ \u001b[39m\u001b[90mStridedViews → StridedViewsCUDAExt\u001b[39m\n",
      "         \u001b[91m  ✗ \u001b[39m\u001b[90mNNlib → NNlibCUDAExt\u001b[39m\n",
      "         \u001b[91m  ✗ \u001b[39mCUDA → SpecialFunctionsExt\n",
      "         \u001b[91m  ✗ \u001b[39m\u001b[90mLinearSolve → LinearSolveCUDAExt\u001b[39m\n",
      "         \u001b[91m  ✗ \u001b[39mCUDA → ChainRulesCoreExt\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.add(\"WAV\")\n",
    "Pkg.add(\"DSP\")\n",
    "\n",
    "using Flux\n",
    "using Flux: onehotbatch, onecold, crossentropy, @epochs\n",
    "using Statistics: mean\n",
    "using WAV         # For loading WAV files\n",
    "using DSP         # For computing spectrograms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Downloads, ZipFile\n",
    "\n",
    "url = \"https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/heads/master.zip\"\n",
    "zip_file = \"fsdd.zip\"\n",
    "Downloads.download(url, zip_file)\n",
    "ZipFile.extract(zip_file, \"fsdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to load audio and compute a spectrogram ---\n",
    "function load_spectrogram(filepath; nfft=1024, hop=512)\n",
    "    # Load audio; WAV.wavread returns (signal, sample_rate)\n",
    "    y, fs = wavread(filepath)\n",
    "    # If stereo, average the channels to mono\n",
    "    if ndims(y) == 2\n",
    "        y = mean(y, dims=2)\n",
    "        y = vec(y)\n",
    "    end\n",
    "    # Compute short-time Fourier transform (STFT)\n",
    "    S = stft(y, nfft=nfft, hop=hop, window=hanning(nfft))\n",
    "    # Take magnitude (you might also want to convert to a Mel scale)\n",
    "    spectro = abs.(S)\n",
    "    # Optionally: resize or normalize the spectrogram here\n",
    "    return spectro\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CNN Model Definition ---\n",
    "# For example, if you convert each audio file into a fixed-size spectrogram,\n",
    "# say 128×128 with 1 channel (grayscale), then set the following dimensions:\n",
    "const IMG_H = 128\n",
    "const IMG_W = 128\n",
    "const CHANNELS = 1      # e.g., grayscale spectrogram\n",
    "\n",
    "# For FSDD, there are 10 classes (digits 0-9)\n",
    "const num_classes = 10\n",
    "\n",
    "model = Chain(\n",
    "    # Input: (IMG_H, IMG_W, CHANNELS)\n",
    "    Conv((3, 3), CHANNELS => 16, relu; pad=(1,1)),\n",
    "    MaxPool((2, 2)),\n",
    "    Conv((3, 3), 16 => 32, relu; pad=(1,1)),\n",
    "    MaxPool((2, 2)),\n",
    "    Conv((3, 3), 32 => 64, relu; pad=(1,1)),\n",
    "    MaxPool((2, 2)),\n",
    "    flatten,\n",
    "    Dense((IMG_H ÷ 8) * (IMG_W ÷ 8) * 64, 128, relu),\n",
    "    Dense(128, num_classes),\n",
    "    softmax\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dummy Data for Demonstration ---\n",
    "# Replace this with your actual data loader that:\n",
    "#   - Iterates over FSDD audio files,\n",
    "#   - Computes their spectrogram using load_spectrogram,\n",
    "#   - Resizes/normalizes to (128, 128, 1),\n",
    "#   - And converts labels (digits) into one-hot vectors.\n",
    "dummy_input = rand(Float32, IMG_H, IMG_W, CHANNELS, 16)  # 16 dummy examples\n",
    "dummy_labels = onehotbatch(rand(1:num_classes, 16), 1:num_classes)\n",
    "\n",
    "# Forward pass example:\n",
    "output = model(dummy_input)\n",
    "println(\"Model output size: \", size(output))\n",
    "\n",
    "# --- Training Loop Skeleton ---\n",
    "loss(x, y) = crossentropy(model(x), y)\n",
    "opt = ADAM()\n",
    "\n",
    "@epochs 5 for (x, y) in [(dummy_input, dummy_labels)]  # Replace with your actual data iterator\n",
    "    grads = gradient(() -> loss(x, y), Flux.params(model))\n",
    "    Flux.Optimise.update!(opt, Flux.params(model), grads)\n",
    "end\n",
    "\n",
    "println(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
